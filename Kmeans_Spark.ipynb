{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Kmeans_Spark.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"39Sdxg9A_Urt"},"source":["# Projet KMEANS en Spark\n","##### Daniel OMOLA & Hy-Boui CHANG"]},{"cell_type":"markdown","metadata":{"id":"Y0sW3AG-_Urw"},"source":["L'objectif du projet est d'explorer les différentes possibilités d'améliorer un programme de KMeans sous Spark.\n","\n","Pour cela, nous utilisons un jeu de données très connu sur les fleurs d'IRIS, et d'appliquer l'algorithme KMeans dessus.  \n","Une première version du programme est fourni, en RDD sous Python.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t14tl_xb_lzN"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"IkaMhmVU_Urx"},"source":["## Chapitre 0 : Description du jeu de données\n","Le jeu de données est disponible sous l'adresse suivante : https://www.dropbox.com/s/9kits2euwawcsj0/iris.data.txt.\n","Ce jeu de données comprend un total de 150 observations, réparties de manière égale entre les trois espèces de fleurs d'iris (setosa, virginica et versicolor). Quatre caractéristiques sont mesurées pour chaque observation (il s'agit de la longueur et la largeur du sépale et du pétale, en centimètres).\n"]},{"cell_type":"code","metadata":{"id":"tMIjIdv-_Ury"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nfr7EIZL_Ur1"},"source":["## Chapitre 1 : analyse du programme de départ (RDD Python)"]},{"cell_type":"markdown","metadata":{"id":"iHrXYMX3_Ur2"},"source":["Afin de décrire le programme KMeans fourni en Python RDD, nous le lançons en local, étape par étape (et non le fichier kmeans-dario-x.py dans son ensemble).\n","\n","Nous avons téléchargé le jeu de données et nous l'avons placé en local.\n","Après avoir lancé Pyspark, il convient de charger le jeu de données :"]},{"cell_type":"code","metadata":{"id":"pWX4tJWV_Ur3"},"source":["lines = sc.textFile(\"iris.data.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J0VYdt0D_Ur5"},"source":["Le jeu de données est modifié / préparé : \n"," -  étape 1 : séparer les éléments identifiés par une virgule\n"," -  étape 2 : mettre les 4 premiers éléments en nombre flottant, puis le type d'iris. Le tout est regroupé ensemble, il s'agit de x[0]\n"," -  étape 3 : ajouter un index à la fin de chaque donnée (début index : 0). On obtient ici x[1]\n"," -  étape 4 : on met permute l'index et les données : donc on inverse x[0] avec x[1]\n"," \n"," Voici un aperçu de ce que l'on obtient : __[(104, [6.5, 3.0, 5.8, 2.2, 'Iris-virginica']), ....__"]},{"cell_type":"code","metadata":{"id":"JSwSsuX1_Ur5"},"source":["data = lines.map(lambda x: x.split(','))\\\n","            .map(lambda x: [float(i) for i in x[:4]] + [x[4] ])\\\n","            .zipWithIndex()\\\n","            .map(lambda x: (x[1],x[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2J2-3-fl_Ur7"},"source":["Dans notre jeu de données, nous savons ici par avance qu'il y a 3 clusters à trouver : setosa, virginica et versicolor.\n","Il s'agit alors ici de choisir au hasard 3 points comme centroïdes potentiels, et d'effectuer les calculs d'ajustement après.\n","A nouveau, on ajouter ici un index commençant par 0, qu'on déplace au début des lignes de données.\n","\n","On obtient par exemple : \n","__[(0, [5.2, 4.1, 1.5, 0.1]),\n"," (1, [6.4, 2.9, 4.3, 1.3]),\n"," (2, [5.1, 3.7, 1.5, 0.4])]__\n","\n","Remarque 1 : la fonction \"takeSample\" admet le paramètre \"withoutReplacement\" car il s'agit de choisir 3 centroïdes distincts.\n","\n","Remarque 2 : si nous ne savions pas à l'avance que notre jeu de données était optimum avec 3 clusters, il aurait fallu lancer .......XXXXX......"]},{"cell_type":"code","metadata":{"id":"uJF9D4up_Ur8"},"source":["nb_clusters=3\n","centroides = sc.parallelize(data.takeSample('withoutReplacment',nb_clusters))\\\n","              .zipWithIndex()\\\n","              .map(lambda x: (x[1],x[0][1][:-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pcZuCp2m_Ur_"},"source":["Nous allons effectuer un premier calcul : le produit cartésian. L'objectif est de coupler chaque ligne de données (150 lignes) avec chacun des 3 centroïdes.\n","Nous remarquons dores et déjà que cette opération prend beaucoup de temps."]},{"cell_type":"code","metadata":{"id":"2Uyd9bkZ_UsA"},"source":["joined = data.cartesian(centroides)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Emetn3lL_UsC"},"source":["Nous allons ensuite calculer la distance euclidienne entre chaque point (chaque donnée avec ses 4 valeurs) et les centroïdes.\n","\n","Nous commençons par définir la fonction de calcul de la distance euclienne, puis nous appliquons cette fonction à notre RDD. Prenons par exemple le point suivant :\n","__((10, [5.4, 3.7, 1.5, 0.2, 'Iris-setosa']), (0, [5.4, 3.4, 1.5, 0.4]))__, \n","\n","et décortiquons la commande\n","__dist = joined.map(lambda x: (x[0][0],(x[1][0], computeDistance(x[0][1][:-1], x[1][1]))))__\n","\n"," - premier élément : x[0][0] = 10 (index de la ligne considérée)\n"," - second élément :   \n","1ère partie : x[1][0] = 0 (c'est l'index du centroïde)  \n","2nde partie : computeDistance(x[0][1][:-1], x[1][1]) (distance euclidienne des 4 chiffres)  \n","\n","A la fin, on obtient en sortie : __(10, (0, 0.360555))__\n"]},{"cell_type":"code","metadata":{"id":"mJX2DRnq_UsC"},"source":["from math import sqrt\n","\n","def computeDistance(x,y):\n","    return sqrt(sum([(a - b)**2 for a,b in zip(x,y)]))\n","\n","dist = joined.map(lambda x: (x[0][0],(x[1][0], computeDistance(x[0][1][:-1], x[1][1]))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-AzGA9xW_UsE"},"source":["La prochaine étape consiste à grouper par l'index de chaque donnée.\n","Pour chaque donnée, on obtient les distances pour les 3 centroïde.\n","\n","Voici un exemple de ce qu'on obtient :\n","__(0, [(0, 0.866025403784438), (1, 3.7), (2, 0.5385164807134504)])__"]},{"cell_type":"code","metadata":{"id":"RHffnIJ0_UsE"},"source":["dist_list = dist.groupByKey().mapValues(list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1jwaNeqZ_UsG"},"source":["Nous allons maintenant calculer le centroïde le plus proche de chaque point.  \n","Pour ce faire, une fonction \"closestCluster\" est créée et son but est de garder pour chaque point, les données du centroïde le plus proche.\n","\n","Nous appliquons la fonction closestCluster à dist_list (la liste des 3 distances). Voici le résultat sur le point d'index 0 :\n"," - en entrée : __(0, [(0, 0.866025403784438), (1, 3.7), (2, 0.5385164807134504)])__\n"," - en sortie : __(0, (2, 0.5385164807134504))__"]},{"cell_type":"code","metadata":{"id":"vcPsgT18_UsG"},"source":["def closestCluster(dist_list):\n","    cluster = dist_list[0][0]  # 0\n","    min_dist = dist_list[0][1] # ? j'aurais mis dist-list[1][1] ?\n","    for elem in dist_list:     # ? j'aurais mis \"for elem in dist_list[1]\"\n","        if elem[1] < min_dist:\n","            cluster = elem[0]\n","            min_dist = elem[1]\n","    return (cluster,min_dist)\n","\n","min_dist = dist_list.mapValues(closestCluster)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RwKgOsgU_UsI"},"source":["Notre objectif est de pouvoir recalculer les 3 nouveaux centroïdes, sachant les points liés aux centroïdes actuels.  \n","Pour cela, nous allons récupérer l'index du centroïde, et les coordonnées de chaque point associés.  \n","\n","assignment ==> nous rapratrions les données complètes du point  \n","clusters ==> nous ne garderons que l'index du centroïde, et les 4 valeurs du point associé (de la ligne de donnée concernée)\n","\n","Voici un exemple de ce que l'on obtient :__(2, [5.1, 3.5, 1.4, 0.2])__  \n","Le centroïde d'index 2, est lié à un point dont les valeurs sont (5.1, 3.5, 1.4, 0.2).  \n","Nous avons toujours nos 150 lignes de données de départ.\n","\n"]},{"cell_type":"code","metadata":{"id":"OviT9Izr_UsI"},"source":["assignment = min_dist.join(data)\n","clusters = assignment.map(lambda x: (x[1][0][0], x[1][1][:-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tBanTxPj_UsK"},"source":["Nous allons pouvoir calculer les nouveaux centroïdes (qui doivent être au barycentre de points assignés à chaque centroïde).\n","\n","count ==> fonction qui ajoute un \"1\" à chaque occurence du même centroïde, et qui les compte, ce que revient à comptabiliser le nbre de lignes de données pour chaque centroïde  \n","somme ==> fonction qui additionne les 4 valeurs d'une donnée, 1 à 1 et par centroide\n","\n","moyenneList ==> appliquée dans le code générant \"centroidesCluster\", il permet de recalculer les \"baricentres\" de l'ensemble des données associés à un centroïde.\n"]},{"cell_type":"code","metadata":{"id":"bW-F5S-d_UsK"},"source":["count = clusters.map(lambda x: (x[0],1)).reduceByKey(lambda x,y: x+y)\n","\n","def sumList(x,y):\n","    return [x[i]+y[i] for i in range(len(x))]\n","\n","somme = clusters.reduceByKey(sumList)\n","\n","def moyenneList(x,n):\n","    return [x[i]/n for i in range(len(x))]\n","\n","centroidesCluster = somme.join(count).map(lambda x : (x[0],moyenneList(x[1][0],x[1][1])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"27V4OarD_UsM"},"source":["## Chapitre 2 : Optimisations"]},{"cell_type":"markdown","metadata":{"id":"8Zwc2B7O_UsM"},"source":["### Modification des paramètres"]},{"cell_type":"code","metadata":{"id":"3Rs9TUru_UsN"},"source":["# graphe pour déterminer le nbre d'itérations optimal\n","# ne pas prendre les 4 données mais peut être que 1 car elles sont toutes correlées ... ==> a faire si on a le temps\n","# hyperparamètres ?? on laisse tomber\n","# passer en array ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GrkI-K___UsO"},"source":["### JOIN et CARTESIAN prennent du temps de calcul"]},{"cell_type":"code","metadata":{"id":"M2Sdxk4W_UsP"},"source":["# filtrer avant de JOIN ou Cartesian"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5UBJ6jv-_UsQ"},"source":["### Programmer en SCALA pour gagner en rapidité d'exécution\n","Spark est implémenté en Java. L’API Python permet de faire beaucoup de choses mais :\n","\n","Elle ne sera jamais aussi complète que l’API Java.\n","\n","Elle sera plus lente que l’API Java ou Scala (car Scala est une surcouche fonctionnelle de Java)."]},{"cell_type":"code","metadata":{"id":"LhTloFYL_UsR"},"source":["val lines = sc.textFile(\"iris.data.txt\")\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FFloaPg-_UsS"},"source":["### Passage en DATAFRAME\n","\n","les RDD (Resilient Distributed Data) sont immutables et compile-time type-safe. They can be used on unstructured data. Lazy.\n","Offre controle et flexibilité.\n","Low-level API\n","Type-safe.\n","RDD : on dit comment faire (et pas ce qu'on veut faire).\n"]},{"cell_type":"code","metadata":{"id":"A7tL9sJ0_UsT"},"source":["# predicate pushdown\n","# QBO\n","# CBO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GY4QGKcg_UsU"},"source":["### Passage en DATASET"]},{"cell_type":"code","metadata":{"id":"td90JPGm_UsV"},"source":["# DatFrame = dataset[Row]\n","# que sur scala ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQUc3w-AOaf-"},"source":["## Data Serialisation Format"]},{"cell_type":"code","metadata":{"id":"vYIJhoviOyCt"},"source":["# Par défaut, Spark utiliser \"Java serialization\" qui est lent.\n","# alternative : utiliser Kryo (plus rapide et plus compact) ou encore mieux on sait que les dataFrame et DataSet utilisent la sérialisation TUNGSTEN.\n","\n","val sparkConf : SparkCond = new SparkConf()\n","  .set(\"spark.serializer\", \"org.apach.spark.serializer.KryoSerializer\")\n","\n","val sparkSession : SparkSession = SparkSession\n","  .builder()\n","  .config(sparkConf)\n","  .getOrCreate()\n","\n","# enregistrement d'une classe custum avec Krio\n","sparkConf.registerKryoClasses(Array(classOf[MyCustomeClass]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_YgfGy8MR4D7"},"source":["## Format de stockage"]},{"cell_type":"code","metadata":{"id":"hLbVtd4jR9ID"},"source":["# utilisation d'un format binaire (plutôt que txt ou csv) \n","# ==> par exemple, utiliser Apache Parquet, Apach Avro\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GCAEouUASU66"},"source":["## Broadcast Join"]},{"cell_type":"code","metadata":{"id":"oRoocavmSX-j"},"source":["val sparkConf : SparkConf = new SparkConf()\n","  .set(\"spark.sql.autoBroadcastJoinThreshold\", \"2147483648\")\n","  .set(\"spark.sql.broadcasTimeout\", \"900\") # modifier le tiemOut par défaut de 300 s\n","\n","# forcer le broadcast\n","val result=bigDataFrame.join(broadcast(smallDataFrame))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dHED9v9_UsY"},"source":["## Amélioration de la mémoire"]},{"cell_type":"code","metadata":{"id":"xsYYTa07_Usa"},"source":["# utilisation du cache()\n","# utilisation de persist()\n","\n","With cache(), you use only the default storage level :\n","   MEMORY_ONLY for RDD\n","   MEMORY_AND_DISK for Dataset\n","   With persist(), you can specify which storage level you want for both RDD and Dataset.\n","\n","Use persist() if you want to assign a storage level other than :\n","   MEMORY_ONLY to the RDD\n","   or MEMORY_AND_DISK for Dataset\n","\n","Which Storage Level to Choose?\n","If your RDDs fit comfortably with the default storage level (MEMORY_ONLY), leave them that way. This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.\n","If not, try using MEMORY_ONLY_SER and selecting a fast serialization library to make the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SWhhqotNflj"},"source":["## Lancement dans le cloud : hardware tuning"]},{"cell_type":"code","metadata":{"id":"8Ii_2AsuTLeI"},"source":["# https://youtu.be/1fEXmuaEGjQ vers 16min\n","# mettre assez d'executor\n","# trop de executor cores ==> trop de parallelisme ==> reduire\n","# pas assez de partitions\n","# persist in memory and disk with serialization\n","# off head memory for caching\n","# pb de données non uniformément distribuées dans les partitions ==> au moment du join, on a un soucis\n","\n","\n","\n","# modification du nbre d'éxecutors\n","--num-executors = 5\n","--executor-cores=14\n","--executor-memory=18GB\n","\n","# dynamic allocation : utiliser autant de ressources que possible selon les besoins\n","\n","# nbre de partitions pour le parallelism\n","\n","# GC Tuning\n","\n","# data locality\n","\n","# dr elephant\n"],"execution_count":null,"outputs":[]}]}